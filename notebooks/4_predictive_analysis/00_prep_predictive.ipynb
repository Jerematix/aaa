{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Demand Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we prepare the taxi trip data for the prediction models by creating dataframes for each spatio-temporal resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read in the cleaned taxi trips data and drop unnecessary columns. Then we rename the remaining columns because we do not need the info pickup anymore in the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['trip_start_timestamp', 'h3_05_pickup', 'h3_06_pickup', 'h3_07_pickup',\n",
       "       'h3_08_pickup', 'h3_09_pickup', 'pickup_centroid', 'temp', 'precip'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('../../data/rides/Taxi_Trips_Cleaned.parquet')\n",
    "\n",
    "df.drop(columns=[\n",
    "    # drop trip specific columns\n",
    "    \"taxi_id\",\n",
    "    \"trip_seconds\",\n",
    "    \"trip_miles\",\n",
    "    \"fare\",\n",
    "    \"tips\",\n",
    "    \"tolls\",\n",
    "    \"Extras\",\n",
    "    \"trip_total\",\n",
    "    \"payment_type\",\n",
    "    \"Company\",\n",
    "    # drop time columns except start timestamp\n",
    "    \"trip_end_timestamp\",\n",
    "    \"hour_end\",\n",
    "    \"4_hour_block_end\",\n",
    "    \"day_end\",\n",
    "    \"week_end\",\n",
    "    \"dayofweek_end\",\n",
    "    \"month_end\",\n",
    "    \"hour_start\",\n",
    "    \"4_hour_block_start\",\n",
    "    \"day_start\",\n",
    "    \"week_start\",\n",
    "    \"dayofweek_start\",\n",
    "    \"month_start\",\n",
    "    \"is_weekday\",\n",
    "    # drop end location columns\n",
    "    \"dropoff_community_area\",\n",
    "    \"dropoff_census_tract\",\n",
    "    \"dropoff_centroid\",\n",
    "    \"h3_05_dropoff\",\n",
    "    \"h3_06_dropoff\",\n",
    "    \"h3_07_dropoff\",\n",
    "    \"h3_08_dropoff\",\n",
    "    \"h3_09_dropoff\",\n",
    "    # drop Pickup Census Tract and Community Area because equal to Pickup Centroid\n",
    "    \"pickup_census_tract\",\n",
    "    \"pickup_community_area\",\n",
    "    # drop datetime because Time Start Timestamp sufficient\n",
    "    \"datetime\"\n",
    "    ],\n",
    "    inplace=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>h3_05</th>\n",
       "      <th>h3_06</th>\n",
       "      <th>h3_07</th>\n",
       "      <th>h3_08</th>\n",
       "      <th>h3_09</th>\n",
       "      <th>centroid</th>\n",
       "      <th>temp</th>\n",
       "      <th>precip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>852664c3fffffff</td>\n",
       "      <td>862664c1fffffff</td>\n",
       "      <td>872664c1effffff</td>\n",
       "      <td>882664c1e1fffff</td>\n",
       "      <td>892664c1e0fffff</td>\n",
       "      <td>POINT (-87.626214906 41.892507781)</td>\n",
       "      <td>-7.0115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 00:30:00</td>\n",
       "      <td>852664c3fffffff</td>\n",
       "      <td>862664c1fffffff</td>\n",
       "      <td>872664c1effffff</td>\n",
       "      <td>882664c1e7fffff</td>\n",
       "      <td>892664c1e73ffff</td>\n",
       "      <td>POINT (-87.63186395 41.892042136)</td>\n",
       "      <td>-7.0115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 00:30:00</td>\n",
       "      <td>852664c3fffffff</td>\n",
       "      <td>862664c1fffffff</td>\n",
       "      <td>872664c1effffff</td>\n",
       "      <td>882664c1e7fffff</td>\n",
       "      <td>892664c1e73ffff</td>\n",
       "      <td>POINT (-87.63186395 41.892042136)</td>\n",
       "      <td>-7.0115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 00:30:00</td>\n",
       "      <td>852664c3fffffff</td>\n",
       "      <td>862664c17ffffff</td>\n",
       "      <td>872664c12ffffff</td>\n",
       "      <td>882664c129fffff</td>\n",
       "      <td>892664c1293ffff</td>\n",
       "      <td>POINT (-87.661265218 41.936159071)</td>\n",
       "      <td>-7.0115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 00:30:00</td>\n",
       "      <td>852664cbfffffff</td>\n",
       "      <td>862664c17ffffff</td>\n",
       "      <td>872664c12ffffff</td>\n",
       "      <td>882664c12dfffff</td>\n",
       "      <td>892664c12dbffff</td>\n",
       "      <td>POINT (-87.675821928 41.935983574)</td>\n",
       "      <td>-7.0115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime            h3_05            h3_06            h3_07   \n",
       "0 2015-01-01 00:00:00  852664c3fffffff  862664c1fffffff  872664c1effffff  \\\n",
       "1 2015-01-01 00:30:00  852664c3fffffff  862664c1fffffff  872664c1effffff   \n",
       "2 2015-01-01 00:30:00  852664c3fffffff  862664c1fffffff  872664c1effffff   \n",
       "3 2015-01-01 00:30:00  852664c3fffffff  862664c17ffffff  872664c12ffffff   \n",
       "4 2015-01-01 00:30:00  852664cbfffffff  862664c17ffffff  872664c12ffffff   \n",
       "\n",
       "             h3_08            h3_09                            centroid   \n",
       "0  882664c1e1fffff  892664c1e0fffff  POINT (-87.626214906 41.892507781)  \\\n",
       "1  882664c1e7fffff  892664c1e73ffff   POINT (-87.63186395 41.892042136)   \n",
       "2  882664c1e7fffff  892664c1e73ffff   POINT (-87.63186395 41.892042136)   \n",
       "3  882664c129fffff  892664c1293ffff  POINT (-87.661265218 41.936159071)   \n",
       "4  882664c12dfffff  892664c12dbffff  POINT (-87.675821928 41.935983574)   \n",
       "\n",
       "     temp  precip  \n",
       "0 -7.0115       0  \n",
       "1 -7.0115       0  \n",
       "2 -7.0115       0  \n",
       "3 -7.0115       0  \n",
       "4 -7.0115       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "df.rename(columns={col:col[:-7] for col in df.columns if \"h3\" in col}, inplace=True) # remove _Pickup from column names\n",
    "df.rename(columns={\"trip_start_timestamp\": \"datetime\", \"pickup_centroid\": \"centroid\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the number of areas for each location discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h3_05: 8\n",
      "h3_06: 24\n",
      "h3_07: 90\n",
      "h3_08: 298\n",
      "h3_09: 437\n",
      "centroid: 442\n"
     ]
    }
   ],
   "source": [
    "location_cols = [\"h3_05\", \"h3_06\", \"h3_07\", \"h3_08\", \"h3_09\", \"centroid\"]\n",
    "\n",
    "## print number of categories per location column\n",
    "for col in location_cols:\n",
    "    print(f\"{col}: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to create a dataframe for a spatio-temporal resolution with features about time, location, weather and points of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spatio_temporal_df(df, poi_df, time_bucket_length, location_bucket, poi_data=True):\n",
    "    # create time bucket\n",
    "    bucket_df = df.copy()\n",
    "    bucket_df[\"time_bucket_floored\"] = bucket_df[\"datetime\"].dt.floor(\n",
    "            f\"{time_bucket_length}H\"\n",
    "    )\n",
    "    bucket_df.drop(columns=[\"datetime\"], inplace=True)\n",
    "\n",
    "    # create demand per time and location bucket\n",
    "    demand_df = bucket_df.groupby([\"time_bucket_floored\", location_bucket]).size().to_frame(\"demand\")\n",
    "    features_df = bucket_df[[\"time_bucket_floored\", location_bucket] + [\"temp\", \"precip\"]].groupby([\"time_bucket_floored\", location_bucket]).mean()\n",
    "    demand_feat_df = features_df.merge(demand_df, left_index=True, right_index=True)\n",
    "    demand_feat_df.reset_index(inplace=True)\n",
    "\n",
    "    # derive time features\n",
    "    demand_feat_df[\"hour_bucket\"] = demand_feat_df[\"time_bucket_floored\"].apply(lambda x: x.hour//time_bucket_length)\n",
    "    demand_feat_df[\"day_of_week\"] = demand_feat_df[\"time_bucket_floored\"].apply(lambda x: x.dayofweek)\n",
    "    demand_feat_df[\"is_weekday\"] = demand_feat_df[\"day_of_week\"] >= 5\n",
    "    demand_feat_df[\"is_weekday\"] = demand_feat_df[\"is_weekday\"].astype(int)\n",
    "    demand_feat_df[\"month\"] = demand_feat_df[\"time_bucket_floored\"].apply(lambda x: x.month)\n",
    "    if time_bucket_length == 24:\n",
    "        demand_feat_df.drop(columns=[\"hour_bucket\"], inplace=True)\n",
    "    demand_feat_df.drop(columns=[\"time_bucket_floored\"], inplace=True)\n",
    "\n",
    "    # get poi data if activated\n",
    "    if poi_data:\n",
    "        poi_location_bucket = poi_df[poi_df[\"h3_res\"] == int(location_bucket[-1])]\n",
    "        poi_location_bucket.drop(columns=[\"h3_res\", \"h3_incl_neighbors\"], inplace=True)\n",
    "        poi_location_bucket.rename(columns={\"h3\": location_bucket}, inplace=True)\n",
    "        return demand_feat_df.merge(poi_location_bucket, on=location_bucket, how=\"left\")\n",
    "\n",
    "    return demand_feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list all time and location resolutions and create for each combination a dataframe which is saved. For the hexagon resolutions we add point of interest data. If a hexagon is not listed in the POI dataframe, its POI values are filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bucket_lengths = [1, 2, 4, 6, 24]\n",
    "location_buckets = [\"h3_05\", \"h3_06\", \"h3_07\", \"h3_08\", \"h3_09\", \"centroid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time bucket length 1 and location bucket h3_05\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 1 and location bucket h3_06\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 1 and location bucket h3_07\n",
      "4 hexagons not in poi data and filled with 0\n",
      "time bucket length 1 and location bucket h3_08\n",
      "23 hexagons not in poi data and filled with 0\n",
      "time bucket length 1 and location bucket h3_09\n",
      "185 hexagons not in poi data and filled with 0\n",
      "time bucket length 1 and location bucket centroid\n",
      "time bucket length 2 and location bucket h3_05\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 2 and location bucket h3_06\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 2 and location bucket h3_07\n",
      "4 hexagons not in poi data and filled with 0\n",
      "time bucket length 2 and location bucket h3_08\n",
      "23 hexagons not in poi data and filled with 0\n",
      "time bucket length 2 and location bucket h3_09\n",
      "185 hexagons not in poi data and filled with 0\n",
      "time bucket length 2 and location bucket centroid\n",
      "time bucket length 4 and location bucket h3_05\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 4 and location bucket h3_06\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 4 and location bucket h3_07\n",
      "4 hexagons not in poi data and filled with 0\n",
      "time bucket length 4 and location bucket h3_08\n",
      "23 hexagons not in poi data and filled with 0\n",
      "time bucket length 4 and location bucket h3_09\n",
      "185 hexagons not in poi data and filled with 0\n",
      "time bucket length 4 and location bucket centroid\n",
      "time bucket length 6 and location bucket h3_05\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 6 and location bucket h3_06\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 6 and location bucket h3_07\n",
      "4 hexagons not in poi data and filled with 0\n",
      "time bucket length 6 and location bucket h3_08\n",
      "23 hexagons not in poi data and filled with 0\n",
      "time bucket length 6 and location bucket h3_09\n",
      "185 hexagons not in poi data and filled with 0\n",
      "time bucket length 6 and location bucket centroid\n",
      "time bucket length 24 and location bucket h3_05\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 24 and location bucket h3_06\n",
      "0 hexagons not in poi data and filled with 0\n",
      "time bucket length 24 and location bucket h3_07\n",
      "4 hexagons not in poi data and filled with 0\n",
      "time bucket length 24 and location bucket h3_08\n",
      "23 hexagons not in poi data and filled with 0\n",
      "time bucket length 24 and location bucket h3_09\n",
      "185 hexagons not in poi data and filled with 0\n",
      "time bucket length 24 and location bucket centroid\n"
     ]
    }
   ],
   "source": [
    "# read poi data\n",
    "poi_df = pd.read_parquet(\"../../data/poi/poi_hexagon_data.parquet\")\n",
    "\n",
    "# create dataframe for each spatio temporal combination\n",
    "for time_bucket_length in time_bucket_lengths:\n",
    "    for location_bucket in location_buckets:\n",
    "        poi = location_bucket != \"centroid\"\n",
    "        print(f\"time bucket length {time_bucket_length} and location bucket {location_bucket}\")\n",
    "        df_st = create_spatio_temporal_df(df, poi_df, time_bucket_length, location_bucket, poi_data=poi)\n",
    "        if poi:\n",
    "            number_hexagons_nan = df_st[df_st[\"public_transport_poi\"].isna()][location_bucket].nunique()\n",
    "            print(f\"{number_hexagons_nan} hexagons not in poi data and filled with 0\")\n",
    "            df_st.fillna(0, inplace=True)\n",
    "        df_st.to_parquet(f\"../../data/predictive/Taxi_Trips_Spatio_Temporal_{time_bucket_length}_{location_bucket}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
