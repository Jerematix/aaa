{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Charging with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a deep Qnetwork agent to regulate the charging of an electric taxi in the time interval of two hours where the charging rate is adjusted every 15 minutes. The maximum charging rate per hour is set to 22kW. After the two hours the battery demand is calculated with a normal distribution with the mean 30kW and sigma 5kW. The charging cost formula is given by a time coefficient alpha, which we set to a constant value here, that is multiplied with e^power where power is the charging rate/4 because we calculate the costs for 15min intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following tutorial for our coding: https://www.section.io/engineering-education/building-a-reinforcement-learning-environment-using-openai-gym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargingEnv(Env):\n",
    "    def __init__(self, max_charging_rate=22, mu=30, sigma=5, alpha_t=0.25):\n",
    "        self.max_charging_rate = max_charging_rate\n",
    "        self.battery_capacity = 100 # maximum battery capacity\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.alpha_t = alpha_t\n",
    "        self.action_space = Discrete(4) # 4 charging actions: zero, low, medium, high\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([self.battery_capacity]))\n",
    "        self.reset_state() # reset battery\n",
    "        self.charging_length = 120 # 120 minutes of charging\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.state = 10 + random.randint(-5,5) # random initial battery between 5 and 15\n",
    "\n",
    "    def step(self, action): \n",
    "        charging_rate = action * (self.max_charging_rate/3) # assuming highest rate is 22 per hour\n",
    "        power = charging_rate * 0.25 # assuming 15min interval\n",
    "        self.state += power  \n",
    "        self.state = min(self.state, self.battery_capacity)\n",
    "        self.charging_length -= 15 # subtract 15min from charging length\n",
    "\n",
    "        # Checking if charging is done\n",
    "        if self.charging_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Calculate charging cost\n",
    "        charging_cost = self.alpha_t * np.exp(power)  # as per the given formula\n",
    "        \n",
    "        # Calculating the reward\n",
    "        if done:\n",
    "            energy_demand = np.random.normal(self.mu, self.sigma)\n",
    "            if self.state < energy_demand:\n",
    "                reward = -10000  # Penalty for running out of energy\n",
    "            else:\n",
    "                # Reward is proportional to the energy saved\n",
    "                reward = -charging_cost\n",
    "        else:\n",
    "            reward = -charging_cost\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Setting the placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Returning the step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reset_state()\n",
    "        self.charging_length = 120 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julialauten/git/aaa/.conda/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = ChargingEnv()\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 24)                48        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 748\n",
      "Trainable params: 748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()    \n",
    "model.add(Dense(24, activation='relu', input_shape=states))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(actions, activation='linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions, warmup_steps=1000):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=warmup_steps, target_model_update=1)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/julialauten/git/aaa/.conda/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 11:22:03.359793: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-08-14 11:22:03.365623: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-08-14 11:22:03.379250: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_2_1/kernel/Assign' id:135 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2_1/kernel, dense_2_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  103/10000 [..............................] - ETA: 14s - reward: -1170.9030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julialauten/git/aaa/.conda/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-08-14 11:22:03.604376: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_2/BiasAdd' id:73 op device:{requested: '', assigned: ''} def:{{{node dense_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_2/MatMul, dense_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-14 11:22:03.615322: W tensorflow/c/c_api.cc:291] Operation '{name:'count_1/Assign' id:242 op device:{requested: '', assigned: ''} def:{{{node count_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_1, count_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  966/10000 [=>............................] - ETA: 13s - reward: -1236.4821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 11:22:05.168861: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_2_1/BiasAdd' id:145 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_2_1/MatMul, dense_2_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-14 11:22:05.241573: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/AddN' id:358 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-14 11:22:05.260789: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/dense/kernel/v/Assign' id:550 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense/kernel/v, training/Adam/dense/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 39s 4ms/step - reward: -594.0151\n",
      "1250 episodes - episode_reward: -4752.121 [-10265.816, -12.509] - loss: 2067529.422 - mae: 2429.103 - mean_q: -2652.008\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: -518.9035\n",
      "done, took 80.208 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x166323520>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions, warmup_steps=1000)\n",
    "dqn.compile(Adam(learning_rate=0.01), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -370.165, steps: 8\n",
      "Episode 2: reward: -489.384, steps: 8\n",
      "Episode 3: reward: -489.384, steps: 8\n",
      "Episode 4: reward: -310.556, steps: 8\n",
      "Episode 5: reward: -489.384, steps: 8\n",
      "-429.77455670036977\n"
     ]
    }
   ],
   "source": [
    "results = dqn.test(env, nb_episodes=5, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintEnv(Env):\n",
    "    def __init__(self, max_charging_rate=22, mu=30, sigma=5, alpha_t=0.25):\n",
    "        self.max_charging_rate = max_charging_rate\n",
    "        self.battery_capacity = 100 # maximum battery capacity\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.alpha_t = alpha_t\n",
    "        self.action_space = Discrete(4) # 4 charging actions: zero, low, medium, high\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([self.battery_capacity]))\n",
    "        self.reset_state() # reset battery\n",
    "        self.charging_length = 120 # 120 minutes of charging\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.state = 10 + random.randint(-5,5) # random initial battery between 5 and 15\n",
    "        print(\"Initial battery level: \", self.state)\n",
    "\n",
    "    def step(self, action): \n",
    "        charging_rate = action * (self.max_charging_rate/3) # assuming highest rate is 22 per hour\n",
    "        power = charging_rate * 0.25 # assuming 15min interval\n",
    "        self.state += power  \n",
    "        self.state = min(self.state, self.battery_capacity)\n",
    "        self.charging_length -= 15 # subtract 15min from charging length\n",
    "\n",
    "        # Checking if charging is done\n",
    "        if self.charging_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Calculate charging cost\n",
    "        charging_cost = self.alpha_t * np.exp(power)  # as per the given formula\n",
    "\n",
    "        print(\"Time left: \", self.charging_length, \"min | Power:\", power.__round__(2), \"| Battery level:\", self.state.__round__(2), \"| Charging cost:\", charging_cost.__round__(2))\n",
    "        \n",
    "        # Calculating the reward\n",
    "        if done:\n",
    "            energy_demand = np.random.normal(self.mu, self.sigma)\n",
    "            if self.state < energy_demand:\n",
    "                reward = -10000  # Penalty for running out of energy\n",
    "                print(\"Running out of energy!\")\n",
    "            else:\n",
    "                # Reward is proportional to the energy saved\n",
    "                reward = -charging_cost\n",
    "                print(\"Charging done!\")\n",
    "        else:\n",
    "            reward = -charging_cost\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Setting the placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Returning the step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reset_state()\n",
    "        self.charging_length = 120 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial battery level:  9\n"
     ]
    }
   ],
   "source": [
    "printEnv = PrintEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial battery level:  7\n",
      "Time left:  105 min | Power: 1.83 | Battery level: 8.83 | Charging cost: 1.56\n",
      "Time left:  90 min | Power: 1.83 | Battery level: 10.67 | Charging cost: 1.56\n",
      "Time left:  75 min | Power: 5.5 | Battery level: 16.17 | Charging cost: 61.17\n",
      "Time left:  60 min | Power: 5.5 | Battery level: 21.67 | Charging cost: 61.17\n",
      "Time left:  45 min | Power: 5.5 | Battery level: 27.17 | Charging cost: 61.17\n",
      "Time left:  30 min | Power: 5.5 | Battery level: 32.67 | Charging cost: 61.17\n",
      "Time left:  15 min | Power: 5.5 | Battery level: 38.17 | Charging cost: 61.17\n",
      "Time left:  0 min | Power: 5.5 | Battery level: 43.67 | Charging cost: 61.17\n",
      "Charging done!\n",
      "Episode 1: Total Reward = -370.1652488722988\n",
      "Initial battery level:  8\n",
      "Time left:  105 min | Power: 1.83 | Battery level: 9.83 | Charging cost: 1.56\n",
      "Time left:  90 min | Power: 5.5 | Battery level: 15.33 | Charging cost: 61.17\n",
      "Time left:  75 min | Power: 5.5 | Battery level: 20.83 | Charging cost: 61.17\n",
      "Time left:  60 min | Power: 5.5 | Battery level: 26.33 | Charging cost: 61.17\n",
      "Time left:  45 min | Power: 5.5 | Battery level: 31.83 | Charging cost: 61.17\n",
      "Time left:  30 min | Power: 5.5 | Battery level: 37.33 | Charging cost: 61.17\n",
      "Time left:  15 min | Power: 5.5 | Battery level: 42.83 | Charging cost: 61.17\n",
      "Time left:  0 min | Power: 5.5 | Battery level: 48.33 | Charging cost: 61.17\n",
      "Charging done!\n",
      "Episode 2: Total Reward = -429.7745567003698\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 2\n",
    "for episode in range(num_episodes):\n",
    "    state = printEnv.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = dqn.forward(state)\n",
    "        next_state, reward, done, _ = printEnv.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Episode {episode+1}: Total Reward = {total_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
