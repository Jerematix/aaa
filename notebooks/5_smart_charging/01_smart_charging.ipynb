{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Charging with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a deep Qnetwork agent to regulate the charging of an electric taxi in the time interval of two hours where the charging rate is adjusted every 15 minutes. The maximum charging rate per hour is set to 22kW. After the two hours the battery demand is calculated with a normal distribution with the mean 30kW and sigma 5kW. The charging cost formula is given by a time coefficient alpha, which we set to a constant value here, that is multiplied with e^power where power is the charging rate/4 because we calculate the costs for 15min intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following tutorial for our coding: https://www.section.io/engineering-education/building-a-reinforcement-learning-environment-using-openai-gym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargingEnv(Env):\n",
    "    def __init__(self, max_charging_rate=22, mu=30, sigma=5, alpha_t=0.25):\n",
    "        self.max_charging_rate = max_charging_rate\n",
    "        self.battery_capacity = 100 # maximum battery capacity\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.alpha_t = alpha_t\n",
    "        self.action_space = Discrete(4) # 4 charging actions: zero, low, medium, high\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([self.battery_capacity]))\n",
    "        self.reset_state() # reset battery\n",
    "        self.charging_length = 120 # 120 minutes of charging\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.state = 10 + random.randint(-5,5) # random initial battery between 5 and 15\n",
    "\n",
    "    def step(self, action): \n",
    "        charging_rate = action * (self.max_charging_rate/3) # assuming highest rate is 22 per hour\n",
    "        power = charging_rate * 0.25 # assuming 15min interval\n",
    "        self.state += power  \n",
    "        self.state = min(self.state, self.battery_capacity)\n",
    "        self.charging_length -= 15 # subtract 15min from charging length\n",
    "\n",
    "        # Checking if charging is done\n",
    "        if self.charging_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Calculate charging cost\n",
    "        charging_cost = self.alpha_t * np.exp(power)  # as per the given formula\n",
    "        \n",
    "        # Calculating the reward\n",
    "        if done:\n",
    "            energy_demand = np.random.normal(self.mu, self.sigma)\n",
    "            if self.state < energy_demand:\n",
    "                reward = -10000  # Penalty for running out of energy\n",
    "            else:\n",
    "                # Reward is proportional to the energy saved\n",
    "                reward = -charging_cost\n",
    "        else:\n",
    "            reward = -charging_cost\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Setting the placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Returning the step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reset_state()\n",
    "        self.charging_length = 120 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julialauten/git/aaa/.venv/lib/python3.9/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = ChargingEnv()\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 24)                48        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 748 (2.92 KB)\n",
      "Trainable params: 748 (2.92 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()    \n",
    "model.add(Dense(24, activation='relu', input_shape=states))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(actions, activation='linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions, warmup_steps=1000):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=warmup_steps, target_model_update=1)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 15:20:34.346308: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-08-14 15:20:34.369958: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2_1/bias/Assign' id:140 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2_1/bias, dense_2_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 80000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  147/10000 [..............................] - ETA: 10s - reward: -485.5162"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julialauten/git/aaa/.venv/lib/python3.9/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-08-14 15:20:34.580918: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2/BiasAdd' id:73 op device:{requested: '', assigned: ''} def:{{{node dense_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_2/MatMul, dense_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-14 15:20:34.589058: W tensorflow/c/c_api.cc:304] Operation '{name:'count_1/Assign' id:242 op device:{requested: '', assigned: ''} def:{{{node count_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_1, count_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step - reward: -749.0919\n",
      "1250 episodes - episode_reward: -5992.735 [-10197.990, -11.196]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "    2/10000 [..............................] - ETA: 33:38 - reward: -31.3683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 15:20:46.211692: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2_1/BiasAdd' id:145 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_2_1/MatMul, dense_2_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-14 15:20:46.296893: W tensorflow/c/c_api.cc:304] Operation '{name:'loss_3/AddN' id:358 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-08-14 15:20:46.311141: W tensorflow/c/c_api.cc:304] Operation '{name:'training/Adam/dense/kernel/v/Assign' id:550 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense/kernel/v, training/Adam/dense/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 34s 3ms/step - reward: -546.2721\n",
      "1250 episodes - episode_reward: -4370.177 [-10256.286, -9.882] - loss: 1868916.460 - mae: 2923.452 - mean_q: -3443.063\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 35s 3ms/step - reward: -517.3915\n",
      "1250 episodes - episode_reward: -4139.132 [-10257.600, -18.099] - loss: 1743536.500 - mae: 2568.784 - mean_q: -2996.502\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: -535.4517\n",
      "1250 episodes - episode_reward: -4283.613 [-10264.503, -14.158] - loss: 1764440.250 - mae: 2325.369 - mean_q: -2650.052\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: -529.6931\n",
      "1250 episodes - episode_reward: -4237.545 [-10307.679, -22.374] - loss: 1887557.375 - mae: 1976.213 - mean_q: -2114.220\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: -521.6521\n",
      "1250 episodes - episode_reward: -4173.217 [-10248.069, -18.099] - loss: 1745184.750 - mae: 1872.284 - mean_q: -2021.708\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: -514.8505\n",
      "1250 episodes - episode_reward: -4118.804 [-10265.816, -23.688] - loss: 1769983.750 - mae: 1827.855 - mean_q: -1953.519\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: -524.2800\n",
      "done, took 265.404 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2852cc790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions, warmup_steps=10000)\n",
    "dqn.compile(Adam(learning_rate=0.01), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=80000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -378.382, steps: 8\n",
      "Episode 2: reward: -489.384, steps: 8\n",
      "Episode 3: reward: -489.384, steps: 8\n",
      "Episode 4: reward: -386.599, steps: 8\n",
      "Episode 5: reward: -489.384, steps: 8\n",
      "Episode 6: reward: -489.384, steps: 8\n",
      "Episode 7: reward: -437.991, steps: 8\n",
      "Episode 8: reward: -489.384, steps: 8\n",
      "Episode 9: reward: -489.384, steps: 8\n",
      "Episode 10: reward: -437.991, steps: 8\n",
      "-457.72660271237527\n"
     ]
    }
   ],
   "source": [
    "results = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(results.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing in new Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training and testing the agent we now create a similar environment that prints out the charging costs and battery stat for each 15min interval to get a closer look into how the agent behaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintEnv(Env):\n",
    "    def __init__(self, max_charging_rate=22, mu=30, sigma=5, alpha_t=0.25):\n",
    "        self.max_charging_rate = max_charging_rate\n",
    "        self.battery_capacity = 100 # maximum battery capacity\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.alpha_t = alpha_t\n",
    "        self.action_space = Discrete(4) # 4 charging actions: zero, low, medium, high\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([self.battery_capacity]))\n",
    "        self.reset_state() # reset battery\n",
    "        self.charging_length = 120 # 120 minutes of charging\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.state = 10 + random.randint(-5,5) # random initial battery between 5 and 15\n",
    "        print(\"Initial battery level: \", self.state)\n",
    "\n",
    "    def step(self, action): \n",
    "        charging_rate = action * (self.max_charging_rate/3) # assuming highest rate is 22 per hour\n",
    "        power = charging_rate * 0.25 # assuming 15min interval\n",
    "        self.state += power  \n",
    "        self.state = min(self.state, self.battery_capacity)\n",
    "        self.charging_length -= 15 # subtract 15min from charging length\n",
    "\n",
    "        # Checking if charging is done\n",
    "        if self.charging_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Calculate charging cost\n",
    "        charging_cost = self.alpha_t * np.exp(power)  # as per the given formula\n",
    "\n",
    "        print(\"Time left: \", self.charging_length, \"min | Power:\", power.__round__(2), \"| Battery level:\", self.state.__round__(2), \"| Charging cost:\", charging_cost.__round__(2))\n",
    "        \n",
    "        # Calculating the reward\n",
    "        if done:\n",
    "            energy_demand = np.random.normal(self.mu, self.sigma)\n",
    "            if self.state < energy_demand:\n",
    "                reward = -10000  # Penalty for running out of energy\n",
    "                print(\"Running out of energy!\")\n",
    "            else:\n",
    "                # Reward is proportional to the energy saved\n",
    "                reward = -charging_cost\n",
    "                print(\"Charging done!\")\n",
    "        else:\n",
    "            reward = -charging_cost\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Setting the placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Returning the step information\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reset_state()\n",
    "        self.charging_length = 120 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial battery level:  7\n",
      "Initial battery level:  5\n",
      "Time left:  105 min | Power: 1.83 | Battery level: 6.83 | Charging cost: 1.56\n",
      "Time left:  90 min | Power: 3.67 | Battery level: 10.5 | Charging cost: 9.78\n",
      "Time left:  75 min | Power: 5.5 | Battery level: 16.0 | Charging cost: 61.17\n",
      "Time left:  60 min | Power: 5.5 | Battery level: 21.5 | Charging cost: 61.17\n",
      "Time left:  45 min | Power: 5.5 | Battery level: 27.0 | Charging cost: 61.17\n",
      "Time left:  30 min | Power: 5.5 | Battery level: 32.5 | Charging cost: 61.17\n",
      "Time left:  15 min | Power: 5.5 | Battery level: 38.0 | Charging cost: 61.17\n",
      "Time left:  0 min | Power: 5.5 | Battery level: 43.5 | Charging cost: 61.17\n",
      "Charging done!\n",
      "Episode 1: Total Reward = -378.381894633853\n",
      "Initial battery level:  14\n",
      "Time left:  105 min | Power: 5.5 | Battery level: 19.5 | Charging cost: 61.17\n",
      "Time left:  90 min | Power: 5.5 | Battery level: 25.0 | Charging cost: 61.17\n",
      "Time left:  75 min | Power: 5.5 | Battery level: 30.5 | Charging cost: 61.17\n",
      "Time left:  60 min | Power: 5.5 | Battery level: 36.0 | Charging cost: 61.17\n",
      "Time left:  45 min | Power: 5.5 | Battery level: 41.5 | Charging cost: 61.17\n",
      "Time left:  30 min | Power: 5.5 | Battery level: 47.0 | Charging cost: 61.17\n",
      "Time left:  15 min | Power: 5.5 | Battery level: 52.5 | Charging cost: 61.17\n",
      "Time left:  0 min | Power: 5.5 | Battery level: 58.0 | Charging cost: 61.17\n",
      "Charging done!\n",
      "Episode 2: Total Reward = -489.3838645284408\n"
     ]
    }
   ],
   "source": [
    "printEnv = PrintEnv()\n",
    "\n",
    "num_episodes = 2\n",
    "for episode in range(num_episodes):\n",
    "    state = printEnv.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = dqn.forward(state)\n",
    "        next_state, reward, done, _ = printEnv.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Episode {episode+1}: Total Reward = {total_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the agent loads mostly with the highest rate of 22kW per hour (5.5kW in 15min) but starts with lower charging rates in the first episode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
